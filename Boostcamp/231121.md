# 일일 회고

## 일일 진도 현황

- [x] DL Basic 5강 (3/3)
- [x] DL Basic 6강 (2/3)
- [x] DL Basic 7강 (3/3)
- [x] DL Basic 8강
- [ ] DL Basic 9강


## 배운 것

### Modern CNN
depth를 깊게, parameter를 줄이고, 성능은 향상

#### ILSVRC
ImageNet Large-Scale Visual Recognition Challenge

#### AlexNet
11x11의 filter를 사용했다.
5 conv, 3 dense

- ReLU
  - preserves properties of linear models
  - easy to optimize with gradient descent
  - overcome the vanishing gradient problem
- 2 GPUs
- data augmentation
- dropout

#### VGGNet
3x3 filter
1x1 conv
dropout(p=-0.5)

> why 3x3?</br>
같은 크기의 receptive field라도 number of parameters에 1.5배 이득을 봤다.

#### GoogLenet
network in network구조

- Inception blocks
  - n x n conv하기 전 1 x 1 conv를 함
  - 전체적인 parameter가 줄어듦
  - 1 x 1 은 채널방향으로 dim을 줄이는 효과가 있다.

#### ResNet
DNN은 train하기 힘들다.
.....


#### DenseNet
resnet에서 더하지말고 concat
concat한 값을 줄이고 늘리고....
?

### Computer Vision Applications

#### Semantic Segmentation
dense classification

pixel 단위로 label을 구분하기

### Recurrent Neural Networks

#### Sequential Model
연속적인 모델!
Input의 수에 제한이 없음

-> Fix the past timespan
- Markov model(first-order autoregressive model)
  - 결과는 직전의 input에만 영향을 받는다.
- Latent autoregressive model
  - Hidden state 가 과거의 정보를 가지고 있음 (summary of the past)

### Transformer
(Attention is all you need)

- The self-attention in both encoder and decoder is the cornerstone of Transformer


## 궁금한 점

## 참고자료

## 할 일 및 계획

- [ ] param 계산하는 법 다시 공부하기
- [ ] 배치사이즈에 따라 sharp minimizers, flat minimizers (클 때, 작을 때)
sharp minimizer < flat minimizer
(아직 이해가 잘 안됨! paper 찾아볼까)
- [ ] stride
- [ ] padding


화욜 dl basic
수욜 data visualization
목욜 전체적인 dl 복습 + 과제
금욜 전체적인 data visu 복습 + 과제
