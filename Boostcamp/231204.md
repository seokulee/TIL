# 일일 회고

## 일일 진도 현황

- [x] [RecSys 이론] 7강 Recommender System with DL 2
  - [x] 7강 퀴즈 (2/3)
- [ ] [RecSys 이론] 8강 Context-aware Reccommendation
  - [ ] 8강 퀴즈

## 배운 것

### Recommender System with GNN

#### Graph

Node와 그 Node를 잇는 Edge를 모아 구성한 자료 구조

일반적으로 그래프는 $G = (V,E)$로 정의함.

#### Graph 사용 이유

- 관계, 상호작용과 같은 추상적인 개념을 다루기에 적합 (유저-아이템 소비관계 등)
- Non-Euclidean Space의 표현 및 학습이 가능

#### GNN (Graph Neural Network)

목적 : 이웃 노드들 간의 정보를 이용해서 특정 노드를 잘 표현할 수 있는 벡터를 잘 찾아내는 것

방법 : 그래프 및 피쳐 데이터를 인접 행렬로 변환하여 MLP에 사용하는 방법등이 있음

노드가 많아지면 연산량이 기하급수적으로 많아지고, 노드의 순서가 바뀌면 의미가 달라질 수 있음

#### GCN (Graph Convolution Network)

위의 한계점을 극복하기위해 연산량을 줄이면서 깊은 네트워크로 간접적인 관계특징까지 추출 가능한 방법 사용

#### Neural Graph Collaborative Filtering

GNN 으로 임베딩 과정에서 인코딩하는 접근법을 제시한 논문

1. 유저와 아이템의 임베딩
2. 상호작용 모델링

- 유저-아이템의 상호작용이 임베딩 단에서부터 학습될 수 있도록 접근
- 유저 ,아이템 개수가 많아질수록 모든 상호작용을 표현하기엔 한계가 존재함 -> GNN 을 통해 High-order Connectivity를 임베딩

#### Ligth GCN

레이어가 깊어질수록 강도가 약해질 것이라는 아이디어를 적용해 모델을 단순화함

학습 파라미터와 연산량이 감소함

feature transformation 이나 nonlinear activation을 제거하고 가중합으로 GCN 적용

### Recommender System with RNN

#### RNN (Recurrent Neural Network)

시퀀스 데이터의 처리와 이해에 좋은 성능을 보이는 신경망 구조로 제안됨

#### LSTM

시퀀스가 길어지는 경우 학습 능력이 현저하게 저하되는 RNN의 단점 극복

장기기억이 전달되었을 떄 forget, 과 input을 통해 output을 결정

#### GRU

LSTM의 변형 중 하나로, 출력 게이트가 따로 없어 파라미터와 연산량이 더 적은 모델 (가벼움)

#### GRU4Rec

고객의 선호는 고정된 것이 아님! -> 지금 고객이 좋아하는 것을 추천

Session이라는 시퀀스를 도입하여 다음에 올 아이템을 추천!

Session Parallel Mini batches

학습
1. 현실에서는 아이템의 수가 많기 때문에 모든 후보 아이템의 확률을 계산하기 어려움
   - 아이템을 negative sampling 하여 subset만으로 loss를 계산함

2. 사용자가 상호작용을 하지 않은 아이템은 조재 자체를 몰랐거나 관심없는 것

### Context-aware Recommendation

유저와 아이템 간 상호작용 뿐만 아니라, 맥락적 정보도 함께 반영하는 추천 시스템

General Predictor에도 사용가능

#### CTR 예측

Click-Through Rate Prediction

-> 유저가 주어진 아이템을 클릭할 확률을 예측하는 문제

#### 로지스틱 회귀

기본 모형

$logit(P(y = 1|x)) = (w_0 + \sum\limits_{i=1}^{n}w_ix_i), w_i \in R)$

#### Dense Feature vs Sparse Feature

- dense feature: 벡터로 표현했을 시 비교적 작은 공간에 밀집되어 분포하는 수치형 변수
- sparse feature: 벡터로 표현했을 시 비교적 넓은 공간에 분포하는 범주형 변수

#### Feature Embedding

One-hot Encoding의 한계

- 파라미터 수가 많아질 수 있음
- 학습 데이터에 등장하는 빈도에 따라 특정 카테고리가 overfitting/underfitting 될 수 있음

그래서 Feature embedding 후 이 피쳐를 가지고 예측하기도 함

### Factorization machines

- 딥러닝 이전엔 서포트 벡터 머신(SVM)이 가장 많이 사용되는 모델이었음
- CF 환경에서는 SVM보다 MF계열 모델이 더 높은 성능을 내왔음
- 하지만 MF모델은 특별한 환경 혹은 데이터에만 적용할 수 있음

### FFM

Field-aware Factirization Machines

FM을 발전시킨 모델로 PITF(Pairwise Interaction Tensor Factorization) 모델에서 아이디어를 얻음

PITF -> user, item, tag의 3개 필드

각가의 서로 다른 latent factor를 정의하여 구함

### Gradient Boosting Machine (GBM)

CTR 예측을 통해 개인화된 추천 시스템을 만들 수 있는 또 다른 대표적인 모델

Boosting : 모델의 편향에 따른 예측오차를 줄이기 휘해 여러 모델을 결합하여 사용하는 기법

decision tree로 된 weak learner들을 연속적으로 학습하여 결합하는 방식

GBM -> gradient descent 사용하여 loss function이 줄어드는 방향으로 weak learnenr 들을 반복적으로 결합함으로써 성능을 향상시키는 Boosting 알고리즘

- 느린 학습 속도
- 과적합 문제

## 궁금한 점

## 참고자료

## 할 일 및 계획
