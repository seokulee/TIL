# 일일 회고

## 일일 진도 현황

- [x] [DL Basic] Transformer 복습
- [x] [RecSys 이론] 4강 Collaborative Filtering 2
  - [x] 4강 퀴즈
  - [x] 기본과제 1
- [x] [RecSys 이론] 5강 item2Vec and ANN
  - [x] 5강 실습
  - [x] 5강 퀴즈
- [ ] [RecSys 이론] 6강 Recommender System with D

## 배운 것

### MF for IMplicit Feedback

#### Alternative Least Square(ALS)

- 유저와 아이템 매트릭스를 번갈아가면서 업데이트
- 두 메트릭스 중 하나를 상수로 놓고 나머지 매트릭스를 업데이트
- 둘 중 하나를 고정하고 다른 하나로 least-square 문제를 푸는 것

Sparse한 데이터에 대해 SGD보다 더 Robust함
대용량 데이트럴 병렬 처리하여 빠른 학습 가능

p와 q를 번갈아가면서 업데이트

#### Bayesian Personalized Ranking(BPR)

>Personalized Ranking<br>
사용자에게 순서가 있는 아이템 리스트를 제공하는

관측되지 않은 데이터에 대해서 아래 두 가지를 모두 고려
- 유저가 아이템에 관심이 없는가?
- 유저가 아이템의 존재를 모르는가?

### Word2Vec

#### Embedding

주어진 데이터를 낮으 차원의 벡터로 만들어서 표현하는 방법

Sparse Representation: 아이템의 전체 가짓수와 차원의 수가 동일
Dense Representation: 아이템의 전체 가짓수보다 훨씬 작은 차원으로 표현

#### Word Embedding

텍스트 분석을 위해 단어를 벡터로 표현하는 방법

단어 간 의미적인 유사도를 구할 수 있음

#### 특징

- 뉴럴 네트워크 기반(기존의 NNLM모델)
- 대량의 문서 데이터 셋을 vector에 투영
- 압축된 형태의 많은 의미를 갖는 dense vector로 표현
- 효율적이고 빠른 학습 가능

#### 학습방법

Continuous Bag of Words(CBOW)

주변의 단어를 사용해 센터의 단어를 예측하는 방법

### Item2Vec

위 Word2Vec의 아이디어를 차용하여 아이템 기반 CF에 적용

단어가 아닌 추천 아이템을 Word2Vec을 사용하여 임베딩

유저가 소비한 `아이템 리스트`를 `문장`으로, `아이템`을 `단어`로 가정하여 Word2Vec사용

아이템을 벡터화하는 것이 최종 목표!!

### Approximate Nearest Neighbor(ANN)

NN : Vector Space Model에서 내가 원하는 Query Vector와 가장 유사한 Vector를 찾는 알고리즘

추천 모델 서빙 : 모델 학습을 통해 구한 유저/아이템의 Vector가 주어질 때, 주어진 Query Vector의 인접한 이웃을 찾아주는 것

#### Brute Force KNN

NN을 정확하게 구하기 위해 나머지 모든 Vector와 유사도 비교를 수행해야 함

-> 정확도를 조금 포기하고 아주 빠른 속도로 주어진 Vector의 근접 이웃을 찾는 방법은 ?

#### ANNOY : spotify에서 개발한 tree-based ANN

주어진 벡터를 subset으로 나누어 tree 형태의 자료 구조로 구성하고 이를 활용하여 탐색함.


## 궁금한 점

## 참고자료

## 할 일 및 계획
