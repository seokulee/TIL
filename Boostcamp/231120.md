# 일일 회고

## 일일 진도 현황

- [x] DL Basic 1강 (2/3)
- [x] DL Basic 2강 (1/3)
- [x] DL Basic 3강 (1/3)
- [x] DL Basic 4강 (2/3)

## 배운 것

선형적이고 일관적인 데이터는 loss를 선형값에서의 거리로 잡을 수 있다. (보통 절대값보다 제곱값을 사용)
loss를 줄이기 위해 역전파 방법을 사용해 loss값을 줄이는 방향으로 간다.

mlp -> hidden layer를 두면 복잡한 문제도 해결할 수 있다. 단순히 선형 결합을 반복하는 것이 아니라, 활성함수(activate function)을 통해 non-linear transform을 거쳐 표현력을 갖게 한다.

### Optimization

#### Generalization
training error와 test error 사이의 gap!
네트워크 성능의 학습데이터에 대한 성능과 유사도!
!= 전체적인 성능

#### Over-fitting vs. Under-fitting
over-fitting : 학습데이터에만 잘 동작함
under-fitting : 학습이 덜 되어서 학습데이터도 잘 못 맞춤
concept적인 이야기!

#### Cross validation
학습데이터로 학습시킨 모델이 학습에 사용되지않은 validate한 데이터에 얼마나 적합한지!
학습데이터로 사용한 데이터는 validate에 두지않음!

#### Bias-variance tradeoff
bias-> 전체적인 평균이 결과값에
variance-> 일관성이 있는지
bias and variance tradeoff


#### Bootstrapping
전체 샘플의 일정 부분만 사용하여 만든 n개의 모델을 만들고 사용한다.

subsample!

#### Bagging and boosting
Bagging : Bootstrapping aggregating
- bootstrapping을 사용해 여러 모델을 만들고 아웃풋에 대한 평균을 냄

Boosting : submodel들을 여러개 만들어 합침!
sum of (weak learner) = strong learner

### Gradient Descent Methods

- Stochastic gradient descent
  - update with single sample

- Mini-batch gradient descent
  - update with subset of data

- Batch gradient descent
  - update with whole data

#### Batch-size Matters

배치사이즈에 따라 sharp minimizers, flat minimizers (클 때, 작을 때)
sharp minimizer < flat minimizer
- [ ] (아직 이해가 잘 안됨! paper 찾아볼까)

#### 종류

- Gradient Descent

gradient를 lr에 곱을 원래 가중치에 빼준다.
(lr 에 영향을 많이 받는다.)

- Momentum

한 번 방향이 정해지면 그 방향에 관성을 준다.(지난 정보도 계속 활용 .. ?)

- Nesterov Accelerated Gradient(NAG)

lookahead gradient 계산해서 사용
-> 현재 정보의 방향으로 한 번 미리 간 뒤 그 정보를 가지고 gradient를 계산 (?????????)

- Adagrad

nn의 parameter 변화율을 적용
학습이 많아질수록 학습이 멈춰짐
이런 문제점을 해결한 방법들 추후에 배움
(adaptive learning)

- Adadelta

timestamp에 대한 변화율을 확인
timestamp만큼 메모리를 더 써야함
no learning rate

- RMSprop

논문 x

- Adam

일반적으로 무난하게 잘 된다.
Adaptive Moment Estimation

### Regularization

학습된 모델이 실제 데이터에도 잘 활용될 수 있도록 하는 방법

- Early Stopping

말 그대로 학습을 멈춤
generalization이 커지기 전에 멈춤

- Parameter Norm Penalty

network parameter들의 전체적인 크기를 작게함
(or Weight decay)

- Data Augmentation

data는 다다익선
dataset을 늘려준다.(이미지 회전, 변환 등)

- Noise Robustness

입력 데이터뿐만 아니라 weight에 noise를 넣는다.

- Label Smoothing

mix-up!
두 label의 decision boundary에 대한 학습
cutout cutmix ...

### Convolution

Continuous convolution

$(f*g)(t) = \int f(\tau)g(t-\tau)d\tau = \int f(t-\tau)g(t)d\tau$

Discrete convolution

$(f*g)(t) = \displaystyle\sum_{i=-\infin}^{\infin} f(i)g(t-i) = \displaystyle\sum_{i=-\infin}^{\infin} f(t-i)g(i)$

2D image convolution

$(I*K)(i,j) = \displaystyle\sum_{m} \displaystyle\sum_{n} I(m,n)K(i-m,j-n) = \displaystyle\sum_{m} \displaystyle\sum_{n} I(i-m,i-n)K(m,n) $

-> 적용하고자 하는 filter를 convolution하면 각 부분마다 blur, emboss, outline등으로 나타나짐.

-> 보통 RGB이미지를 convolution 하면 filter크기의 feature map이 나옴

CNN 만드는 방향 -> deep하지만 parameter를 줄이는 방향으로 발전

#### stride
(다시 정리하기)

#### padding
(다시 정리하기)

#### Convolution Arithmetic

Padding (1), Stride (1), 3 X 3 Kernel

### 퀴즈

회귀 -> MSE(Mean Squared Error)
분류 -> CE(Cross Entrophy)
확률 -> MLE(Maximum Likelyhood Error)

학습률 (learning rate)가 너무 높으면 가중치 업데이트가 너무 커져  최적의 값에 수렴하지 않고 발산할 가능성이 있다.

MLP에서 은닉층은 복잡한 문제 해결 능력을 향상시키기 위해 사용된다.


## 궁금한 점

gradient methods 필요한 논문 찾아보...기?

Local minimum에서 벗어나기 위해 다음 보기 중 가장 적절한 방법은 무엇인가요?
학습률 증가
가중치 초기화(?)
모멘텀 사용
다 가능하지 않나요..?

## 참고자료


## 할 일 및 계획

